{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184194"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./hate_speech_binary_dataset.csv\")\n",
    "data[\"문장\"] = data[\"문장\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
    "data[\"문장\"] = data[\"문장\"].replace(\"\", np.nan)\n",
    "data.dropna(inplace=True)\n",
    "data.drop_duplicates(subset=[\"문장\"], inplace=True)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"문장\"], data[\"혐오 여부\"],\n",
    "                                                    test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ㅎㅏㄴ', 'ㄱㅡㄹ')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jamo_split(s:str) -> str:\n",
    "    CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    JONGSUNG_LIST = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "    s_code = ord(s)\n",
    "\n",
    "    if '가' <= s <= '힣':\n",
    "        return CHOSUNG_LIST[(s_code - 0xAC00) // (28 * 21)] \\\n",
    "               + JUNGSUNG_LIST[((s_code - 0xAC00) // 28) % 21] \\\n",
    "               + JONGSUNG_LIST[(s_code - 0xAC00) % 28]\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "\n",
    "jamo_split(\"한\"), jamo_split(\"글\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅇㅏㄴㄴㅕㅇㅎㅏㅅㅔㅇㅛ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_jamo_split(s:str) -> str:\n",
    "    result = \"\"\n",
    "    for i in list(s.strip()):\n",
    "        result += jamo_split(i)\n",
    "\n",
    "    return result\n",
    "\n",
    "string_jamo_split(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_jamo_split = []\n",
    "for i in X_train:\n",
    "    X_train_jamo_split.append(string_jamo_split(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㅋㅋㅋㅋㅋㅋ ㅂㅕㅇㅅㅣㄴㅅㅐㄲㅣ ㄱㅐㅉㅗㄹㄹㅣㄴㅗ ㅇㅣㄱㅏㅇㅑㅋㅋㅋㅋㅋㅋㅋㅋㅋ',\n",
       " 'ㅇㅣㄹㄱㅔㅇㅣㅇㅕ ㅇㅕㄹㅅㅏㄱㅏ ㄷㅚㅇㅓㄹㅏ',\n",
       " 'ㅈㅗㅅㅓㄴㄴㅏㅁㄴㅕㅅㅏㅇㅇㅕㄹㅈㅣㅅㅏㅅㅡㅋㅐㄴㄷㅡㄹ ㅇㅣㅎㅜ ㅊㅚㄱㅗㅇㅢ ㅍㅠㅈㅓㄴㅅㅏㄱㅡㄱㅁㅔㄹㄹㅗㅁㅜㄹㅇㅣㅇㅓㅆㄷㅏㄷㅐㅎㅏㄴㅁㅣㄴㄱㅜㄱ ㅇㅣㄴㄷㅣㅇㅕㅇㅎㅘ ㅎㅘㅇㅣㅌㅣㅇ',\n",
       " 'ㅇㅣㄱㅁㅕㅇ ㄴㅗㅃㅏ ㅂㅜㄴㅌㅏㅇㅊㅜㅇㅅㅐㄲㅣ ㅂㅜㄷㅡㄹㅂㅜㄷㅡㄹ ㅁㅗㅅㅊㅏㅁㄱㅗ ㅅㅡㅁㅓㄹㅅㅡㅁㅓㄹ ㄱㅣㅇㅓㄴㅏㅇㅘㅆㅈㅛ ㅋㅋㅋ',\n",
       " 'ㅈㅒㄴ ㄸㅗ ㄴㅜㄱㅜㅇㅑ',\n",
       " 'ㅅㅂㅈㅣㄴㅉㅏ ㄱㅗㄹㄸㅐㄹㅣㄴㅡㄴ ㄴㅏㄹㅏㄴㅔ',\n",
       " 'ㅇㅘㄴㅈㅓㄴ ㅊㅚㄱㅗ ㄷㅡㄹㅏㅁㅏ ㅂㅔㅅㅡㅌㅡㅂㅔㅅㅡㅌㅡ',\n",
       " 'ㄴㅏㄴ ㅁㅝㄴㅈㅣㄷㅗ ㅁㅗㄹㅡㄱㅗ ㅊㅓㅁㅇㅔ ㅇㅑㅆㅏ ㅇㅕㄴㄱㅕㄹ ㅅㅣㅋㅕㅈㅜㄴㅡㄴ ㄹㅣㅇㅋㅡ ㅇㅣㄴㅈㅜㄹ ㅇㅏㄹㄱㅗ ㄷㅡㄱㅏㅆㄴㅡㄴㄷㅔ ㅉㅏㅇㅋㅟㅂㅓㄹㄹㅔ ㅁㅏㄹ ㄸㅡㄱㅣㄹㄹㅐ ㅉㅏㅈㅡㅇㄴㅏㅆㄴㅡㄴㄷㅔ ㅇㅣㄹㅓㄴㄱㅓㅇㅕㅆㄴㅗ',\n",
       " 'ㅈㅐㅁㅣㅆㄴㅔㅇㅛ ㅎ ㅍㅕㅇㅈㅓㅁㅇㅣ ㄴㅓㅁㅜ ㄴㅏㅈㅇㅏㅇㅛ',\n",
       " 'ㅁㅏㅁㅁㅏㅁㅁㅏ ㅁㅣㅊㅣㄴㅅㅐㄲㅣㄷㅡㄹ',\n",
       " 'ㅇㅣㅇㅑ ㄱㅘㄴㅅㅏㅇㅇㅣ ㅈㅗㅎㄴㅔ ㅎㅗㄴㅏㅁㅎㅕㅇㅇㅣㄴㅔ ㅅㅏㄹㅇㅏㅇㅣㅆㄴㅔ',\n",
       " 'ㅁㅏㄴㄴㅏㅁㄱㅘ ㅎㅔㅇㅓㅈㅣㅁ ㄱㅡㄹㅣㄱㅗ ㅈㅐㅎㅚ',\n",
       " 'ㅋㅣㄹㄹㅣㅇㅇㅛㅇㅇㅡㄹㅗ ㅂㅗㄹㅁㅏㄴㅇㅝㄴㅈㅏㄱㄱㅘ ㄷㅏㄹㅡㄱㅗ ㅂㅜㅈㅏㄱㅇㅡㄹㅗ ㅇㅔㄴㄷㅣㅇㅇㅡㄹ ㄴㅐㅂㅓㄹㅕㅅㅓ ㅇㅏㅅㅟㅂㄴㅔ ㅅㅐㄷㅡㅇㅔㄴㄷㅣㅇ ㅅㅟㅅ',\n",
       " 'ㄷㅓㄱㅂㅜㄴㅇㅔ ㅇㅜㄹㅣㄴㅏㄹㅏ ㅇㅘㅇㅅㅣㄹ ㄷㅗㅈㅏㄱㅣㄱㅏ ㅁㅏㄴㄷㅡㄹㅇㅓㅈㅣㄴㅡㄴ ㅂㅜㄴㅇㅝㄴㅇㅢ ㅁㅗㅅㅡㅂㄱㅘ ㄱㅏㅅㅡㅁㅇㅏㅍㅡㄴ ㅈㅗㅅㅓㄴㅇㅢ ㅇㅕㄱㅅㅏ ㄱㅡㄹㅣㄱㅗ ㅇㅣㄹㅝㅈㅣㅈㅣ ㅁㅗㅅㅎㅏㄴ ㅅㅡㄹㅍㅡㄴ ㅅㅏㄹㅏㅇㅇㅣㅇㅑㄱㅣㄲㅏㅈㅣㅇㅘㄴㅂㅕㄱㅎㅏㄴ ㅋㅐㅅㅡㅌㅣㅇㅂㅜㅌㅓ ㅇㅕㄴㄱㅣㄲㅏㅈㅣㅈㅐㅁㅣㅇㅣㅆㄱㅔ ㅈㅏㄹㅂㅘㅆㅅㅡㅂㄴㅣㄷㅏ ㄱㅏㅁㅅㅏㅎㅏㅂㄴㅣㄷㅏ ㅅㅜㄱㅗㅁㅏㄶㅇㅡㅅㅕㅆㅇㅓㅇㅛ',\n",
       " 'ㄷㅏㅅㅣㅂㅗㄱㅗㅅㅣㅍㄴㅔㅇㅛ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_jamo_split[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(X_train_jamo_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array(tokenizer.texts_to_sequences(X_train_jamo_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(x) for x in encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = pad_sequences(encoded, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(500, 100))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(80, return_sequences=True)))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(80)))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.add(keras.layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor=\"loss\", mode=\"min\", verbose=1, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(encoded, y_train, epochs=15, callbacks=[es], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(new_sentence):\n",
    "    # 0에 가까울수록 악성 댓글임\n",
    "    new_sentence = string_jamo_split(new_sentence)\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "    pad_new = pad_sequences(encoded, maxlen = 500)\n",
    "    return float(model.predict(pad_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
